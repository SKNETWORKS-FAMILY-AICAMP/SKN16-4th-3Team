# í†µí•© Streamlit + LLM í¼ìŠ¤ë„ì»¬ëŸ¬ ì±—ë´‡ ì˜ˆì œ (RAG ì ìš© ë²„ì „)
# ì‹¤í–‰: streamlit run personal_color_chat_ui.py
# ì¤€ë¹„: os.getenv["OPENAI_API_KEY"]ì— OpenAI API í‚¤ ì„¤ì •
# ë°ì´í„° íŒŒì¼ì€ data/RAG í´ë”ì— ìœ„ì¹˜:
#   - data/RAG/personal_color_RAG.txt (ë¶ˆë³€ ì§€ì‹: í¼ìŠ¤ë„ì»¬ëŸ¬ ì´ë¡ )
#   - data/RAG/beauty_trend_2025_autumn_RAG.txt (ê°€ë³€ ì§€ì‹: 2025 ê°€ì„ ìµœì‹  íŠ¸ë Œë“œ)

import os
from dotenv import load_dotenv
# .env íŒŒì¼ ë¡œë“œ
load_dotenv()
# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œ ì‚¬ìš©
from openai import OpenAI
client = OpenAI(api_key=OPENAI_API_KEY)
import json
import time
import streamlit as st
from typing import Dict, Any, Tuple, List
from math import sqrt

# ======================================================================
# íŒŒíŠ¸ A) ê³µìš© ìœ í‹¸ë¦¬í‹° (RAGìš© í…ìŠ¤íŠ¸ ë¶„í• /ì„ë² ë”©/ê²€ìƒ‰)
# ======================================================================

def chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì¼ì • ê¸¸ì´ë¡œ ì•ˆì „í•˜ê²Œ ë¶„í• """
    if overlap >= chunk_size:
        raise ValueError("overlapì€ chunk_sizeë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤.")
    chunks = []
    start = 0
    text_length = len(text)
    while start < text_length:
        end = min(start + chunk_size, text_length)
        chunks.append(text[start:end])
        if end == text_length:
            break
        start += (chunk_size - overlap)  # ì•ˆì „í•œ ì „ì§„
    return [c.strip() for c in chunks if c.strip()]

def cosine_similarity(a: List[float], b: List[float]) -> float:
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    dot = sum(x * y for x, y in zip(a, b))
    na = sqrt(sum(x * x for x in a)) or 1e-8
    nb = sqrt(sum(x * x for x in b)) or 1e-8
    return dot / (na * nb)

def embed_texts(client: OpenAI, texts: List[str], model: str = "text-embedding-3-small") -> List[List[float]]:
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    res = client.embeddings.create(model=model, input=texts)
    return [item.embedding for item in res.data]

def build_rag_index(client: OpenAI, filepath: str, chunk_size: int = 800, overlap: int = 100) -> Dict[str, Any]:
    """RAG ì¸ë±ìŠ¤ ìƒì„±"""
    with open(filepath, encoding="utf-8") as f:
        text = f.read()
    chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)
    embeddings = embed_texts(client, chunks)
    return {"chunks": chunks, "embeddings": embeddings}

def top_k_chunks(query: str, index: Dict[str, Any], client: OpenAI, k: int = 3) -> List[str]:
    """ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ Top-K ë°˜í™˜"""
    if not index or "embeddings" not in index or "chunks" not in index:
        return []
    q_emb = embed_texts(client, [query])[0]
    sims = [(cosine_similarity(q_emb, emb), i) for i, emb in enumerate(index["embeddings"])]
    sims.sort(reverse=True, key=lambda x: x[0])
    return [index["chunks"][i] for _, i in sims[:k]]

# ======================================================================
# íŒŒíŠ¸ B) ë°ì´í„°/ìƒíƒœ í´ë˜ìŠ¤ ë° LLM ì±—ë´‡ ë¡œì§
# ======================================================================

class ChatState:
    INITIAL = "initial"
    COLLECTING = "collecting"
    ANALYZING = "analyzing"
    COMPLETE = "complete"

class PersonalColorResult:
    """í¼ìŠ¤ë„ì»¬ëŸ¬ ë¶„ì„ ê²°ê³¼ ë°ì´í„° ê°ì²´"""
    def __init__(self, primary_tone: str, sub_tone: str, description: str, recommendations: List[str]):
        self.primary_tone = primary_tone
        self.sub_tone = sub_tone
        self.description = description
        self.recommendations = recommendations

    def to_dict(self) -> Dict[str, Any]:
        return {
            "primary_tone": self.primary_tone,
            "sub_tone": self.sub_tone,
            "description": self.description,
            "recommendations": self.recommendations
        }

class LLMPersonalColorChatbot:
    """LLM ê¸°ë°˜ í¼ìŠ¤ë„ì»¬ëŸ¬ ì±—ë´‡"""
    def __init__(self, client: OpenAI, model: str = "gpt-4o-mini"):
        self.client = client
        self.model = model
        self.state: str = ChatState.INITIAL
        self.collected_data: Dict[str, Any] = {"answers": []}
        self.last_question: str = ""

    def start_new_session(self) -> str:
        self.state = ChatState.COLLECTING
        self.collected_data = {"answers": []}
        self.last_question = ""
        return "ì•ˆë…•í•˜ì„¸ìš”! ğŸ¨ í¼ìŠ¤ë„ì»¬ëŸ¬ ì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤. ê°„ë‹¨í•œ ì§ˆë¬¸ì„ ë“œë¦´ê²Œìš”. ì¤€ë¹„ë˜ì…¨ë‚˜ìš”?"

    def _call_llm(self, messages: List[Dict[str, str]], temperature: float = 0.7, max_tokens: int = 400):
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content
        except Exception:
            return None

    def process_message(self, user_message: str, temperature: float = 0.7, max_tokens: int = 400) -> str:
        timestamp = int(time.time())
        self.collected_data.setdefault("answers", []).append({"text": user_message, "timestamp": timestamp})

        prompt_system = "ë‹¹ì‹ ì€ í¼ìŠ¤ë„ì»¬ëŸ¬ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ..."
        prompt_user = (
            f"ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘ëœ ì •ë³´: {self.collected_data}\n"
            f"ì‚¬ìš©ìì˜ ìµœì‹  ë‹µë³€: {user_message}\n\n"
            "ë‹¤ìŒ ë‹¨ê³„ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°ˆ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
        )

        messages = [{"role": "system", "content": prompt_system}, {"role": "user", "content": prompt_user}]
        answer = self._call_llm(messages, temperature=temperature, max_tokens=max_tokens)
        if answer is None:
            return "âš ï¸ ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        return answer

    def get_final_report(self, temperature: float = 0.3, max_tokens: int = 800) -> Tuple[str, PersonalColorResult]:
        answers = [a.get("text", "") for a in self.collected_data.get("answers", [])]
        query = " / ".join(answers).strip() or "í¼ìŠ¤ë„ì»¬ëŸ¬ ì§„ë‹¨ ê¸°ì¤€"

        fixed_index = st.session_state.get("fixed_index")
        trend_index = st.session_state.get("trend_index")

        fixed_chunks = top_k_chunks(
            query, fixed_index, self.client, k=st.session_state.get("top_k", 3)
        ) if fixed_index else []

        trend_chunks = top_k_chunks(
            query, trend_index, self.client, k=st.session_state.get("top_k", 3)
        ) if trend_index else []

        # âœ… í”„ë¡¬í”„íŠ¸ ê°•í™”: ë¶ˆë³€/ê°€ë³€ ì§€ì‹ êµ¬ë¶„í•´ì„œ ë¦¬í¬íŠ¸ì— ë°˜ì˜í•˜ë„ë¡ ì§€ì‹œ
        prompt_system = (
            "ë‹¹ì‹ ì€ í¼ìŠ¤ë„ì»¬ëŸ¬ ì „ë¬¸ê°€ì´ì ìµœì‹  íŒ¨ì…˜ íŠ¸ë Œë“œ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. "
            "ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë¶„ì„í•˜ê³ , í¼ìŠ¤ë„ì»¬ëŸ¬ ì´ë¡ (ê¸°ë³¸ ì›ì¹™)ê³¼ ìµœì‹  íŠ¸ë Œë“œ(ë³´ì™„ ì„¤ëª…)ë¥¼ "
            "ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•´ í•˜ë‚˜ì˜ ì™„ì„±ëœ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”. "
            "ë”±ë”±í•œ ë‚˜ì—´ì´ ì•„ë‹ˆë¼, ì „ë¬¸ê°€ê°€ ì¡°ì–¸í•˜ë“¯ ë¶€ë“œëŸ½ê²Œ ì‘ì„±í•˜ì„¸ìš”. "
            "JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."
        )

        prompt_user = (
            "[ê´€ë ¨ ì§€ì‹ ì»¨í…ìŠ¤íŠ¸]\n"
            + "\n".join(fixed_chunks + trend_chunks) + "\n\n"
            f"[ì‚¬ìš©ì ë‹µë³€ ìš”ì•½]\n{query}\n\n"
            "ë°˜ë“œì‹œ ë‹¤ìŒ JSON í•„ë“œë§Œ í¬í•¨í•´ ë°˜í™˜í•˜ì„¸ìš”:\n"
            "- primary_tone: 'ì›œ' ë˜ëŠ” 'ì¿¨'\n"
            "- sub_tone: 'ë´„'/'ì—¬ë¦„'/'ê°€ì„'/'ê²¨ìš¸'\n"
            "- description: í¼ìŠ¤ë„ì»¬ëŸ¬ ê¸°ë³¸ ì›ì¹™ê³¼ ìµœì‹  íŠ¸ë Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•œ ì„¤ëª… (ì™„ì„±ëœ ë¬¸ì¥)\n"
            "- recommendations: 3~6ê°œì˜ ì§§ì€ ë¬¸ì¥í˜• ì¶”ì²œ (ì˜ˆ: 'ë¸Œë¼ìš´ ë‹ˆíŠ¸ëŠ” ë”°ëœ»í•œ ì¸ìƒì„ ì¤ë‹ˆë‹¤.', "
            "'ë¸”ë™ ì½”íŠ¸ëŠ” ì„¸ë ¨ëœ ë¶„ìœ„ê¸°ë¥¼ ë”í•´ì¤ë‹ˆë‹¤.')\n\n"
            "ì¶œë ¥ ì˜ˆì‹œ:\n"
            "{\n"
            "  \"primary_tone\": \"ì›œ\",\n"
            "  \"sub_tone\": \"ê°€ì„\",\n"
            "  \"description\": \"ê°€ì„ ì›œí†¤ì—ëŠ” ë”°ëœ»í•œ ë¸Œë¼ìš´ ê³„ì—´ì´ ì˜ ì–´ìš¸ë¦½ë‹ˆë‹¤. ë§ˆì¹¨ 2025ë…„ ê°€ì„ íŠ¸ë Œë“œë„ ë¸Œë¼ìš´ê³¼ ë¸”ë™ì´ ì¤‘ì‹¬ì´ë‹ˆ, ê¸°ë³¸ ì›ì¹™ê³¼ íŠ¸ë Œë“œë¥¼ í•¨ê»˜ ì‚´ë ¤ ìŠ¤íƒ€ì¼ë§í•´ ë³´ì„¸ìš”.\",\n"
            "  \"recommendations\": [\n"
            "    \"ë¸Œë¼ìš´ ë‹ˆíŠ¸ëŠ” ë”°ëœ»í•œ ì¸ìƒì„ ì¤ë‹ˆë‹¤.\",\n"
            "    \"ë¸”ë™ ì½”íŠ¸ëŠ” ì„¸ë ¨ëœ ë¶„ìœ„ê¸°ë¥¼ ë”í•´ì¤ë‹ˆë‹¤.\",\n"
            "    \"ì¹´ë©œìƒ‰ ê°€ë°©ì€ í¬ì¸íŠ¸ ì•„ì´í…œìœ¼ë¡œ ì í•©í•©ë‹ˆë‹¤.\"\n"
            "  ]\n"
            "}\n"
        )

        messages = [
            {"role": "system", "content": prompt_system},
            {"role": "user", "content": prompt_user}
        ]

        content = self._call_llm(messages, temperature=temperature, max_tokens=max_tokens)


        if content is None:
            return "âš ï¸ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜", None

        start, end = content.find("{"), content.rfind("}")
        if start == -1 or end == -1 or end <= start:
            return "âš ï¸ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", None

        try:
            data = json.loads(content[start:end+1])
        except json.JSONDecodeError:
            return "âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜", None

        primary = data.get("primary_tone", "")
        sub = data.get("sub_tone", "")
        description = data.get("description", "")
        recommendations = data.get("recommendations", [])
        # íƒ€ì… ë³´ì •
        if not isinstance(recommendations, list):
            if isinstance(recommendations, str):
                recommendations = [r.strip() for r in recommendations.split(",") if r.strip()]
            else:
                recommendations = []

        result = PersonalColorResult(primary, sub, description, recommendations)
        final_message = f"âœ¨ í¼ìŠ¤ë„ì»¬ëŸ¬ ì§„ë‹¨ ê²°ê³¼: {result.primary_tone} - {result.sub_tone} âœ¨\n\n{result.description}"
        return final_message, result

# ======================================================================
# íŒŒíŠ¸ C) Streamlit ì•± ì´ˆê¸°í™” ë° ì„¸ì…˜ ìƒíƒœ
# ======================================================================

st.set_page_config(page_title="í¼ìŠ¤ë„ì»¬ëŸ¬ ì±—ë´‡ (LLM+RAG)", page_icon="ğŸ¨", layout="centered")
st.title("ğŸ¨ í¼ìŠ¤ë„ì»¬ëŸ¬ ì±—ë´‡ (LLM + RAG ì—°ë™)")
st.markdown("í¼ìŠ¤ë„ì»¬ëŸ¬ ì´ë¡ (ë¶ˆë³€ ì§€ì‹)ê³¼ ìµœì‹  íŠ¸ë Œë“œ(ê°€ë³€ ì§€ì‹)ë¥¼ ê²°í•©í•´ ë§ì¶¤ ë¦¬í¬íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
if os.getenv("OPENAI_API_KEY") is None:
    st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. os.getenv['OPENAI_API_KEY']ì— í‚¤ë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.")
    st.stop()

try:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
except Exception as e:
    st.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

# ë°ì´í„° ê²½ë¡œ
DATA_DIR = os.path.join("data", "RAG")
FIXED_PATH = os.path.join(DATA_DIR, "personal_color_RAG.txt")
TREND_PATH = os.path.join(DATA_DIR, "beauty_trend_2025_autumn_RAG.txt")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "pc_chatbot" not in st.session_state:
    st.session_state.pc_chatbot = LLMPersonalColorChatbot(client=client)
    st.session_state.messages = []
    st.session_state.last_report = None
    st.session_state.show_report_expander = False
    st.session_state.is_dialog_complete = False
    st.session_state.temperature = 0.7
    st.session_state.max_tokens = 400
    first_question = st.session_state.pc_chatbot.start_new_session()
    st.session_state.messages.append({"role": "assistant", "content": first_question})

# ======================================================================
# íŒŒíŠ¸ D) ì‚¬ì´ë“œë°”: ì„¤ì • + RAG ì¸ë±ìŠ¤ ë¹Œë“œ/ë¦¬ì…‹
# ======================================================================

with st.sidebar:
    st.header("ì„¤ì •")
    st.session_state.temperature = st.slider(
        "Temperature", min_value=0.0, max_value=1.0,
        value=st.session_state.get("temperature", 0.7), step=0.05
    )
    st.session_state.max_tokens = st.number_input(
        "Max tokens", min_value=50, max_value=2000,
        value=st.session_state.get("max_tokens", 400), step=50
    )

    st.markdown("---")
    st.subheader("RAG ì˜µì…˜")
    st.session_state.chunk_size = st.number_input(
        "Chunk size", min_value=200, max_value=2000,
        value=st.session_state.get("chunk_size", 800), step=100
    )
    st.session_state.overlap = st.number_input(
        "Chunk overlap", min_value=0, max_value=500,
        value=st.session_state.get("overlap", 100), step=50
    )
    st.session_state.top_k = st.slider(
        "Top-K ê²€ìƒ‰ ê°œìˆ˜", min_value=1, max_value=10,
        value=st.session_state.get("top_k", 3), step=1
    )

    st.markdown("---")
    st.subheader("RAG ì¸ë±ìŠ¤")
    if st.button("RAG ì¸ë±ìŠ¤ ìƒì„±/ê°±ì‹ "):
        try:
            st.session_state.fixed_index = build_rag_index(
                client, FIXED_PATH,
                chunk_size=st.session_state.chunk_size,
                overlap=st.session_state.overlap
            )
            st.session_state.trend_index = build_rag_index(
                client, TREND_PATH,
                chunk_size=st.session_state.chunk_size,
                overlap=st.session_state.overlap
            )
            st.success("RAG ì¸ë±ìŠ¤ê°€ ìƒì„±/ê°±ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.rerun()
        except Exception as e:
            st.error(f"ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

    fixed_ready = "fixed_index" in st.session_state
    trend_ready = "trend_index" in st.session_state
    st.caption(f"ë¶ˆë³€ ì§€ì‹ ì¸ë±ìŠ¤: {'âœ… ì¤€ë¹„ë¨' if fixed_ready else 'âŒ ì—†ìŒ'}")
    st.caption(f"ê°€ë³€ ì§€ì‹ ì¸ë±ìŠ¤: {'âœ… ì¤€ë¹„ë¨' if trend_ready else 'âŒ ì—†ìŒ'}")

    st.markdown("---")
    st.subheader("í…ŒìŠ¤íŠ¸/ë””ë²„ê·¸")
    if st.button("ê°•ì œë¡œ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"):
        st.session_state.pc_chatbot.state = ChatState.ANALYZING
        # âœ… í† í° ì—¬ìœ ë¥¼ ë„‰ë„‰íˆ í™•ë³´ (ìµœì†Œ 800)
        max_tokens_val = max(800, st.session_state.max_tokens)
        final_message, result = st.session_state.pc_chatbot.get_final_report(
            temperature=st.session_state.temperature,
            max_tokens=max_tokens_val
        )
        if result:
            st.session_state.last_report = result.to_dict()
            st.session_state.is_dialog_complete = True
            st.session_state.messages.append({"role": "assistant", "content": final_message})
            st.success("ìµœì¢… ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.error(final_message)

    if st.button("ìƒˆ ëŒ€í™” ì‹œì‘"):
        st.session_state.pc_chatbot.start_new_session()
        st.session_state.messages = []
        msg = st.session_state.pc_chatbot.start_new_session()
        st.session_state.messages.append({"role": "assistant", "content": msg})
        st.session_state.last_report = None
        st.session_state.show_report_expander = False
        st.session_state.is_dialog_complete = False
        st.rerun()

# ======================================================================
# íŒŒíŠ¸ E) ì±„íŒ… UI ë° ëŒ€í™” íë¦„
# ======================================================================

# âœ… ìƒë‹¨ ì•ˆë‚´ë¬¸ ì¶”ê°€
st.info("ğŸ’¡ ì–¸ì œë“ ì§€ ì±„íŒ…ì°½ì— **'ë¦¬í¬íŠ¸ ìƒì„±'** ì´ë¼ê³  ì…ë ¥í•˜ë©´ ì¦‰ì‹œ ìµœì¢… ì§„ë‹¨ ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

input_disabled = st.session_state.is_dialog_complete
placeholder = "ì§„ë‹¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. 'ìƒˆ ëŒ€í™” ì‹œì‘'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”." if input_disabled else "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."

if user_input := st.chat_input(placeholder, disabled=input_disabled):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    chatbot: LLMPersonalColorChatbot = st.session_state.pc_chatbot

    # âœ… ì‹œë™ì–´ 'ë¦¬í¬íŠ¸ ìƒì„±' ê°ì§€
    if user_input.strip() == "ë¦¬í¬íŠ¸ ìƒì„±":
        chatbot.state = ChatState.ANALYZING
        max_tokens_val = max(800, st.session_state.max_tokens)
        final_message, result_obj = chatbot.get_final_report(
            temperature=st.session_state.temperature,
            max_tokens=max_tokens_val
        )
        if result_obj:
            st.session_state.last_report = result_obj.to_dict()
            st.session_state.is_dialog_complete = True
            st.session_state.show_report_expander = True
            st.session_state.messages.append({"role": "assistant", "content": final_message})
            with st.chat_message("assistant"):
                st.markdown(final_message)
        else:
            st.session_state.messages.append({"role": "assistant", "content": final_message})
            with st.chat_message("assistant"):
                st.markdown(final_message)
        st.rerun()

    else:
        # âœ… ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ (COMPLETE ê°ì§€ ì œê±°ë¨)
        bot_response = chatbot.process_message(
            user_input,
            temperature=st.session_state.temperature,
            max_tokens=st.session_state.max_tokens
        )
        st.session_state.messages.append({"role": "assistant", "content": bot_response})
        with st.chat_message("assistant"):
            st.markdown(bot_response)

    st.rerun()

# ======================================================================
# íŒŒíŠ¸ F) ì™„ë£Œ UI: ë¦¬í¬íŠ¸ ìƒì„¸ ë³´ê¸° ë° ì¬ì‹œì‘
# ======================================================================

if st.session_state.is_dialog_complete and st.session_state.last_report:
    with st.expander("ğŸ” í¼ìŠ¤ë„ì»¬ëŸ¬ ë¦¬í¬íŠ¸", expanded=st.session_state.show_report_expander):
        report = st.session_state.last_report

        # í†¤ ì •ë³´ëŠ” ê°•ì¡° ì¹´ë“œ ìŠ¤íƒ€ì¼ë¡œ
        st.markdown(
            f"""
            <div style="padding:12px; border-radius:8px; background-color:#f0f2f6; margin-bottom:12px;">
                <b>ê¸°ë³¸ í†¤:</b> {report.get('primary_tone','-')}<br>
                <b>ì„¸ë¶€ í†¤:</b> {report.get('sub_tone','-')}
            </div>
            """,
            unsafe_allow_html=True
        )

        # ì „ë¬¸ê°€ ì§„ë‹¨ì€ ë³¸ë¬¸ ìŠ¤íƒ€ì¼
        st.markdown("### AI ì „ë¬¸ê°€ ì§„ë‹¨")
        st.markdown(f"> {report.get('description','-')}")

        # ì¶”ì²œ ìŠ¤íƒ€ì¼ì€ ì¹´ë“œ ë¦¬ìŠ¤íŠ¸ ëŠë‚Œìœ¼ë¡œ
        st.markdown("### ì¶”ì²œ ìŠ¤íƒ€ì¼")
        recs = report.get("recommendations", [])
        if recs:
            for rec in recs:
                st.markdown(
                    f"""
                    <div style="padding:10px; margin:6px 0; border-left:4px solid #4CAF50; background-color:#fafafa; border-radius:4px;">
                        {rec}
                    </div>
                    """,
                    unsafe_allow_html=True
                )
        else:
            st.markdown("- (ì¶”ì²œ ì—†ìŒ)")

    if st.button("ìƒˆ ëŒ€í™” ì‹œì‘"):
        st.session_state.pc_chatbot.start_new_session()
        st.session_state.messages = []
        msg = st.session_state.pc_chatbot.start_new_session()
        st.session_state.messages.append({"role": "assistant", "content": msg})
        st.session_state.last_report = None
        st.session_state.show_report_expander = False
        st.session_state.is_dialog_complete = False
        st.rerun()

if not st.session_state.messages:
    msg = st.session_state.pc_chatbot.start_new_session()
    st.session_state.messages.append({"role": "assistant", "content": msg})
    st.rerun()