import os
from dotenv import load_dotenv
# .env íŒŒì¼ ë¡œë“œ
load_dotenv()
# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œ ì‚¬ìš©
from openai import OpenAI
client = OpenAI(api_key=OPENAI_API_KEY)
import io
import json
import time
import re
import streamlit as st
from typing import List, Dict, Any, Tuple
from math import sqrt

# ---------------- ìœ í‹¸ í•¨ìˆ˜ ----------------
def chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:
    if overlap >= chunk_size:
        raise ValueError("overlapì€ chunk_sizeë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤.")
    chunks = []
    start = 0
    text_length = len(text)
    while start < text_length:
        end = min(start + chunk_size, text_length)
        chunks.append(text[start:end])
        if end == text_length:
            break
        # âœ… ë¬´í•œë£¨í”„ ë°©ì§€: startë¥¼ ì•ìœ¼ë¡œ ì „ì§„
        start += (chunk_size - overlap)
    return [c.strip() for c in chunks if c.strip()]

def cosine_similarity(a: List[float], b: List[float]) -> float:
    dot = sum(x*y for x,y in zip(a,b))
    na = sqrt(sum(x*x for x in a)) or 1e-8
    nb = sqrt(sum(x*x for x in b)) or 1e-8
    return dot / (na * nb)

def embed_texts(client: OpenAI, texts: List[str], model: str = "text-embedding-3-small") -> List[List[float]]:
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    res = client.embeddings.create(model=model, input=texts)
    return [item.embedding for item in res.data]

def top_k_chunks(query: str, index: Dict[str, Any], client: OpenAI, k: int = 5) -> List[str]:
    """ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ Top-K ë°˜í™˜"""
    q_emb = embed_texts(client, [query])[0]
    sims = [(cosine_similarity(q_emb, emb), i) for i, emb in enumerate(index["embeddings"])]
    sims.sort(reverse=True, key=lambda x: x[0])
    return [index["chunks"][i] for _, i in sims[:k]]

# ---------------- RAG ì´ˆê¸°í™” ----------------
def build_rag_index(client: OpenAI, filepath: str) -> Dict[str, Any]:
    """txt íŒŒì¼ì„ ì½ì–´ ì²­í¬ ë¶„í•  í›„ ì„ë² ë”© ì¸ë±ìŠ¤ ìƒì„±"""
    with open(filepath, encoding="utf-8") as f:
        text = f.read()
    chunks = chunk_text(text, chunk_size=800, overlap=100)
    embeddings = embed_texts(client, chunks)
    return {"chunks": chunks, "embeddings": embeddings}

# ---------------- LLM + RAG ë¦¬í¬íŠ¸ ìƒì„± ----------------
def generate_report_with_rag(client: OpenAI, user_answers: List[str],
                             fixed_index: Dict[str, Any], trend_index: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:
    """ì‚¬ìš©ì ë‹µë³€ + RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
    query = " / ".join(user_answers)

    # ë¶ˆë³€ ì§€ì‹ì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
    fixed_chunks = top_k_chunks(query, fixed_index, client, k=3) if fixed_index else []
    # ê°€ë³€ ì§€ì‹ì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
    trend_chunks = top_k_chunks(query, trend_index, client, k=3) if trend_index else []

    prompt_system = (
        "ë‹¹ì‹ ì€ í¼ìŠ¤ë„ì»¬ëŸ¬ ì „ë¬¸ê°€ì´ì ìµœì‹  íŒ¨ì…˜ íŠ¸ë Œë“œ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë¶„ì„í•˜ê³ , í¼ìŠ¤ë„ì»¬ëŸ¬ ì´ë¡ (ë¶ˆë³€ ì§€ì‹)ê³¼ ìµœì‹  íŠ¸ë Œë“œ(ê°€ë³€ ì§€ì‹)ë¥¼ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ "
        "ìµœì¢… ì§„ë‹¨ ë¦¬í¬íŠ¸ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."
    )

    prompt_user = f"""
ì‚¬ìš©ì ë‹µë³€ ìš”ì•½:
{query}

[ë¶ˆë³€ ì§€ì‹ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸]
{chr(10).join(fixed_chunks)}

[ê°€ë³€ ì§€ì‹ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸]
{chr(10).join(trend_chunks)}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì„¸ìš”:
- primary_tone: 'ì›œ' ë˜ëŠ” 'ì¿¨'
- sub_tone: 'ë´„'/'ì—¬ë¦„'/'ê°€ì„'/'ê²¨ìš¸'
- description: í¼ìŠ¤ë„ì»¬ëŸ¬ íŠ¹ì§• ì„¤ëª… (ë¶ˆë³€ ì§€ì‹ ê¸°ë°˜)
- recommendations: ì¶”ì²œ ìƒ‰ìƒ/ìŠ¤íƒ€ì¼ (ìµœì‹  íŠ¸ë Œë“œ ë°˜ì˜)
"""

    messages = [
        {"role": "system", "content": prompt_system},
        {"role": "user", "content": prompt_user}
    ]

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.3,
        max_tokens=600
    )
    content = response.choices[0].message.content

    # JSON ì¶”ì¶œ
    start = content.find("{")
    end = content.rfind("}")
    if start == -1 or end == -1:
        return "âš ï¸ JSON ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", None

    try:
        data = json.loads(content[start:end+1])
    except json.JSONDecodeError:
        return "âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜", None

    final_message = f"âœ¨ í¼ìŠ¤ë„ì»¬ëŸ¬ ì§„ë‹¨ ê²°ê³¼: {data.get('primary_tone')} - {data.get('sub_tone')} âœ¨\n\n{data.get('description')}"
    return final_message, data

# ---------------- Streamlit ë°ëª¨ ----------------
st.title("ğŸ¨ í¼ìŠ¤ë„ì»¬ëŸ¬ ì±—ë´‡ (RAG ì ìš© ë°ëª¨)")

if os.getenv("OPENAI_API_KEY") is None:
    raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# RAG ì¸ë±ìŠ¤ ìƒì„± (ì•± ì‹œì‘ ì‹œ 1íšŒ)
if "fixed_index" not in st.session_state:
    st.session_state.fixed_index = build_rag_index(client, "data/RAG/personal_color_RAG.txt")
if "trend_index" not in st.session_state:
    st.session_state.trend_index = build_rag_index(client, "data/RAG/beauty_trend_2025_autumn_RAG.txt")

# ì‚¬ìš©ì ì…ë ¥
user_input = st.text_input("í¼ìŠ¤ë„ì»¬ëŸ¬ ì§„ë‹¨ì„ ìœ„í•´ ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: í”¼ë¶€ê°€ ë…¸ë¥´ìŠ¤ë¦„í•˜ê³  ê°ˆìƒ‰ ë¨¸ë¦¬ë¥¼ ìì£¼ ì—¼ìƒ‰í•´ìš”)")

if st.button("ë¦¬í¬íŠ¸ ìƒì„±") and user_input:
    answers = [user_input]
    final_message, report = generate_report_with_rag(
        client,
        answers,
        st.session_state.fixed_index,
        st.session_state.trend_index
    )
    st.write(final_message)
    if report:
        st.json(report)